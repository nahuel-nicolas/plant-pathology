{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiments\n",
    "Testing multiple architectures on a subset of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_DIR = Path('data/train')\n",
    "EXPERIMENTS_DIR = Path('data/experiments')\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "IMG_SIZE = 224\n",
    "VAL_SPLIT = 0.2\n",
    "EXPERIMENT_SAMPLE_SIZE = 0.2  # 20% of training data\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiments Dataset\n",
    "Copy 20% of images from each class in train to experiments folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiments directory if it doesn't exist\n",
    "if not EXPERIMENTS_DIR.exists():\n",
    "    EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f'Created experiments directory: {EXPERIMENTS_DIR}')\n",
    "    \n",
    "    # Copy 20% of images from each class to experiments\n",
    "    for class_name in CLASS_NAMES:\n",
    "        train_class_dir = TRAIN_DIR / class_name\n",
    "        exp_class_dir = EXPERIMENTS_DIR / class_name\n",
    "        exp_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get all images in this class\n",
    "        all_images = list(train_class_dir.glob('*.jpg'))\n",
    "        \n",
    "        # Calculate number of images to copy (20%)\n",
    "        num_exp = int(len(all_images) * EXPERIMENT_SAMPLE_SIZE)\n",
    "        \n",
    "        # Randomly select images for experiments\n",
    "        exp_images = random.sample(all_images, num_exp)\n",
    "        \n",
    "        # Copy images to experiments folder\n",
    "        for img_path in exp_images:\n",
    "            dest_path = exp_class_dir / img_path.name\n",
    "            shutil.copy2(str(img_path), str(dest_path))\n",
    "        \n",
    "        print(f'{class_name}: Copied {num_exp} images to experiments ({len(all_images)} total in train)')\n",
    "    \n",
    "    print(f'\\nExperiments dataset created successfully!')\n",
    "else:\n",
    "    print(f'Experiments directory already exists: {EXPERIMENTS_DIR}')\n",
    "    # Show experiments set statistics\n",
    "    for class_name in CLASS_NAMES:\n",
    "        exp_class_dir = EXPERIMENTS_DIR / class_name\n",
    "        if exp_class_dir.exists():\n",
    "            num_exp_images = len(list(exp_class_dir.glob('*.jpg')))\n",
    "            print(f'{class_name}: {num_exp_images} experiment images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generators\n",
    "Using experiments folder as training set with 20% validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantPathologyDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Data augmentation and normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Prepare experiment data\n",
    "exp_image_paths = []\n",
    "exp_labels = []\n",
    "\n",
    "for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "    class_dir = EXPERIMENTS_DIR / class_name\n",
    "    for img_path in class_dir.glob('*.jpg'):\n",
    "        exp_image_paths.append(str(img_path))\n",
    "        exp_labels.append(class_idx)\n",
    "\n",
    "print(f'Experiment images: {len(exp_image_paths)}')\n",
    "\n",
    "# Split into train and validation (20% validation)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    exp_image_paths, exp_labels, test_size=VAL_SPLIT, random_state=42, stratify=exp_labels\n",
    ")\n",
    "\n",
    "print(f'Train split: {len(train_paths)}')\n",
    "print(f'Validation split: {len(val_paths)}')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PlantPathologyDataset(train_paths, train_labels, train_transform)\n",
    "val_dataset = PlantPathologyDataset(val_paths, val_labels, val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f'Created dataloaders: train={len(train_loader)} batches, val={len(val_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_loader, val_loader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train a model and log results to tensorboard\n",
    "    \"\"\"\n",
    "    # Setup tensorboard writer\n",
    "    writer = SummaryWriter(f'runs/{model_name}')\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc='Training'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc='Validation'):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, f'best_{model_name}.pth')\n",
    "            print(f'Best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    writer.close()\n",
    "    print(f'\\n{model_name} training completed! Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('Training ResNet50')\n",
    "print('='*70)\n",
    "\n",
    "# Load pretrained ResNet50\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "num_features = resnet50.fc.in_features\n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "resnet50 = resnet50.to(DEVICE)\n",
    "print(f'ResNet50 loaded and moved to {DEVICE}')\n",
    "\n",
    "# Train\n",
    "resnet50_acc = train_model(resnet50, 'ResNet50', train_loader, val_loader, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: EfficientNet-B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('Training EfficientNet-B1')\n",
    "print('='*70)\n",
    "\n",
    "# Load pretrained EfficientNet-B1\n",
    "efficientnet_b1 = models.efficientnet_b1(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in efficientnet_b1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "num_features = efficientnet_b1.classifier[1].in_features\n",
    "efficientnet_b1.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "efficientnet_b1 = efficientnet_b1.to(DEVICE)\n",
    "print(f'EfficientNet-B1 loaded and moved to {DEVICE}')\n",
    "\n",
    "# Train\n",
    "efficientnet_b1_acc = train_model(efficientnet_b1, 'EfficientNet-B1', train_loader, val_loader, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: EfficientNet-B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('Training EfficientNet-B2')\n",
    "print('='*70)\n",
    "\n",
    "# Load pretrained EfficientNet-B2\n",
    "efficientnet_b2 = models.efficientnet_b2(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in efficientnet_b2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "num_features = efficientnet_b2.classifier[1].in_features\n",
    "efficientnet_b2.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "efficientnet_b2 = efficientnet_b2.to(DEVICE)\n",
    "print(f'EfficientNet-B2 loaded and moved to {DEVICE}')\n",
    "\n",
    "# Train\n",
    "efficientnet_b2_acc = train_model(efficientnet_b2, 'EfficientNet-B2', train_loader, val_loader, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('Training VGG16')\n",
    "print('='*70)\n",
    "\n",
    "# Load pretrained VGG16\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "num_features = vgg16.classifier[6].in_features\n",
    "vgg16.classifier[6] = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "vgg16 = vgg16.to(DEVICE)\n",
    "print(f'VGG16 loaded and moved to {DEVICE}')\n",
    "\n",
    "# Train\n",
    "vgg16_acc = train_model(vgg16, 'VGG16', train_loader, val_loader, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: DINOv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('Training DINOv2')\n",
    "print('='*70)\n",
    "\n",
    "# Load pretrained DINOv2 (using torch.hub)\n",
    "dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "\n",
    "# Freeze all layers\n",
    "for param in dinov2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add classifier\n",
    "class DINOv2Classifier(nn.Module):\n",
    "    def __init__(self, dinov2_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.dinov2 = dinov2_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, num_classes)  # DINOv2 ViT-S/14 outputs 384 features\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.dinov2(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "dinov2_model = DINOv2Classifier(dinov2, NUM_CLASSES).to(DEVICE)\n",
    "print(f'DINOv2 loaded and moved to {DEVICE}')\n",
    "\n",
    "# Train\n",
    "dinov2_acc = train_model(dinov2_model, 'DINOv2', train_loader, val_loader, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('FINAL RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "print(f'ResNet50:         {resnet50_acc:.2f}%')\n",
    "print(f'EfficientNet-B1:  {efficientnet_b1_acc:.2f}%')\n",
    "print(f'EfficientNet-B2:  {efficientnet_b2_acc:.2f}%')\n",
    "print(f'VGG16:            {vgg16_acc:.2f}%')\n",
    "print(f'DINOv2:           {dinov2_acc:.2f}%')\n",
    "print('='*70)\n",
    "\n",
    "# Find best model\n",
    "results = {\n",
    "    'ResNet50': resnet50_acc,\n",
    "    'EfficientNet-B1': efficientnet_b1_acc,\n",
    "    'EfficientNet-B2': efficientnet_b2_acc,\n",
    "    'VGG16': vgg16_acc,\n",
    "    'DINOv2': dinov2_acc\n",
    "}\n",
    "best_model = max(results, key=results.get)\n",
    "print(f'\\nBest performing model: {best_model} ({results[best_model]:.2f}%)')\n",
    "print('\\nTo view tensorboard logs, run:')\n",
    "print('tensorboard --logdir=runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
